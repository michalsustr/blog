<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="http://michal.sustr.sk/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://michal.sustr.sk/blog/" rel="alternate" type="text/html" /><updated>2016-11-11T10:32:33+01:00</updated><id>http://michal.sustr.sk/blog/</id><title>My blog</title><subtitle>Personal website of Michal Šustr - a soul between research and engineering. Artificial Intelligence / Machine Learning.</subtitle><author><name>Michal Šustr</name><uri>http://michal.sustr.sk</uri></author><entry><title>Why I think Schmidhuber’s conclusion of non-interfering AI is wrong</title><link href="http://michal.sustr.sk/blog/why-i-think-schmidhubers-conclusion-of-non-interfering-ai-is-wrong/" rel="alternate" type="text/html" title="Why I think Schmidhuber's conclusion of non-interfering AI is wrong" /><published>2016-08-11T00:00:00+02:00</published><updated>2016-08-11T00:00:00+02:00</updated><id>http://michal.sustr.sk/blog/why-i-think-schmidhubers-conclusion-of-non-interfering-ai-is-wrong</id><content type="html" xml:base="http://michal.sustr.sk/blog/why-i-think-schmidhubers-conclusion-of-non-interfering-ai-is-wrong/">&lt;p&gt;I really enjoyed watching &lt;a href=&quot;https://www.youtube.com/watch?v=XkltShNd6XE&quot;&gt;Prof. Jürgen Schmidhuber’s talk about future of AI&lt;/a&gt;. The topic of AI becoming Terminator-style is an evergreen and as a person who actually works in research I start to get quite annoyed about these futurology predictions. I think they are similarly annoying as quantum physics interpretations of The Secret movie - mathematical truths twisted by knaves to make a trap for fools (yes I like Kipling ;-)&lt;/p&gt;

&lt;p&gt;His argument is that super AI will not be concerned with humans that much as we are not concerned with for example ants, because our goals are different. The only time we really become aware of ants and think about them is when they come inside the house and annoy us in the kitchen. Super AI would be somewhat similar to this.&lt;/p&gt;

&lt;p&gt;That makes a lot of sense. The problem is that this AI would have a lot of friction points with humans, because it would compete for scarce resources. Machines that we build today are built out of relatively scarce materials, and presumably one of the goals of an AI like this would be to spread itself (like life). Which it couldn’t do if it needs scarce metallic resources and needs to compete with humans for them. Once it could move to a different, non-competitive place, like the Moon, where it could find these materials, there’s no reason for interference.&lt;/p&gt;

&lt;p&gt;But anyway, I think discussions on these topics are still quite irrelevant today, because all we can do today is to build some statistical models that have become a bit more advanced than in the past.&lt;/p&gt;

&lt;p&gt;Just my two cents.&lt;/p&gt;</content><author><name>Michal Šustr</name><uri>http://michal.sustr.sk</uri></author><summary>I really enjoyed watching Prof. Jürgen Schmidhuber’s talk about future of AI. The topic of AI becoming Terminator-style is an evergreen and as a person who actually works in research I start to get quite annoyed about these futurology predictions. I think they are similarly annoying as quantum physics interpretations of The Secret movie - mathematical truths twisted by knaves to make a trap for fools (yes I like Kipling ;-)

His argument is that super AI will not be concerned with humans that much as we are not concerned with for example ants, because our goals are different. The only time we really become aware of ants and think about them is when they come inside the house and annoy us in the kitchen. Super AI would be somewhat similar to this.

That makes a lot of sense. The problem is that this AI would have a lot of friction points with humans, because it would compete for scarce resources. Machines that we build today are built out of relatively scarce materials, and presumably one of the goals of an AI like this would be to spread itself (like life). Which it couldn’t do if it needs scarce metallic resources and needs to compete with humans for them. Once it could move to a different, non-competitive place, like the Moon, where it could find these materials, there’s no reason for interference.

But anyway, I think discussions on these topics are still quite irrelevant today, because all we can do today is to build some statistical models that have become a bit more advanced than in the past.

Just my two cents.</summary></entry><entry><title>Protocol buffers - python tips</title><link href="http://michal.sustr.sk/blog/python/protocol-buffers-python-tips/" rel="alternate" type="text/html" title="Protocol buffers - python tips" /><published>2016-07-21T00:00:00+02:00</published><updated>2016-07-21T00:00:00+02:00</updated><id>http://michal.sustr.sk/blog/python/protocol-buffers-python-tips</id><content type="html" xml:base="http://michal.sustr.sk/blog/python/protocol-buffers-python-tips/">&lt;p&gt;&lt;a href=&quot;https://developers.google.com/protocol-buffers/&quot;&gt;Protocol buffers&lt;/a&gt; (protobufs) are well known Google’s mechanism for a language-neutral, platform-neutral, extensible mean for serializing structured data.&lt;/p&gt;

&lt;p&gt;Basically, it’s exchange format that is smaller than JSON/XML/whatever because it is saved in binary. And you can use code generators to make it very simple to use it in different languages.&lt;/p&gt;

&lt;p&gt;To make it to work quickly, just get the binary release from &lt;a href=&quot;https://github.com/google/protobuf/releases&quot;&gt;official relases&lt;/a&gt; and put the &lt;code class=&quot;highlighter-rouge&quot;&gt;protoc&lt;/code&gt; command to &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/protoc&lt;/code&gt;. Once you write your own .proto spec file, like &lt;a href=&quot;https://github.com/google/protobuf/blob/master/examples/addressbook.proto&quot;&gt;the addressbook in examples&lt;/a&gt; you can compile it with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ protoc --python_out=. *.proto
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The docs for how to create the .proto specs are well documented on their website, so no need to explain it here. Maybe just a few things to keep in mind when you write your protocol headers:&lt;/p&gt;

&lt;p&gt;Extending protobufs:
- you must not change the tag numbers of any existing fields.
- you must not add or delete any required fields.
- you may delete optional or repeated fields.
- you may add new optional or repeated fields but you must use fresh tag numbers (i.e. tag numbers that were never used in this protocol buffer, not even by deleted fields).&lt;/p&gt;

&lt;p&gt;Currently there are two active versions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/proto2&quot;&gt;proto2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/proto2&quot;&gt;proto3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;proto2&lt;/strong&gt; version is still actively supported, probably mainly because I find the &lt;strong&gt;proto3&lt;/strong&gt; kind of &lt;em&gt;sucks&lt;/em&gt; :-) (one thing I didn’t like it doesn’t have support for &lt;em&gt;default values&lt;/em&gt;. As it turns out, &lt;a href=&quot;https://groups.google.com/forum/#!topic/protobuf/ZRpcfmeGK6s&quot;&gt;I’m not the only one who doesn’t like it&lt;/a&gt; but &lt;a href=&quot;https://stackoverflow.com/questions/33222551/why-are-there-no-custom-default-values-in-proto3&quot;&gt;it has reasons&lt;/a&gt; ).&lt;/p&gt;

&lt;p&gt;In python I struggled for an hour to find a way how to create human readable .prototxt files. All you have to do is (extending the python examples &lt;a href=&quot;https://github.com/google/protobuf/blob/master/examples/add_person.py&quot;&gt;add_person.py&lt;/a&gt; and &lt;a href=&quot;https://github.com/google/protobuf/blob/master/examples/list_people.py&quot;&gt;list_people.p&lt;/a&gt; )&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# import this&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;google.protobuf&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_format&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# and then use this to read&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text_format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;address_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# and this to write&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text_format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MessageToString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;address_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Enjoy.&lt;/p&gt;</content><author><name>Michal Šustr</name><uri>http://michal.sustr.sk</uri></author><summary>Protocol buffers (protobufs) are well known Google’s mechanism for a language-neutral, platform-neutral, extensible mean for serializing structured data.

Basically, it’s exchange format that is smaller than JSON/XML/whatever because it is saved in binary. And you can use code generators to make it very simple to use it in different languages.

To make it to work quickly, just get the binary release from official relases and put the protoc command to /usr/bin/protoc. Once you write your own .proto spec file, like the addressbook in examples you can compile it with

$ protoc --python_out=. *.proto



The docs for how to create the .proto specs are well documented on their website, so no need to explain it here. Maybe just a few things to keep in mind when you write your protocol headers:

Extending protobufs:
- you must not change the tag numbers of any existing fields.
- you must not add or delete any required fields.
- you may delete optional or repeated fields.
- you may add new optional or repeated fields but you must use fresh tag numbers (i.e. tag numbers that were never used in this protocol buffer, not even by deleted fields).

Currently there are two active versions:


  proto2
  proto3


The proto2 version is still actively supported, probably mainly because I find the proto3 kind of sucks :-) (one thing I didn’t like it doesn’t have support for default values. As it turns out, I’m not the only one who doesn’t like it but it has reasons ).

In python I struggled for an hour to find a way how to create human readable .prototxt files. All you have to do is (extending the python examples add_person.py and list_people.p )

# import this
from google.protobuf import text_format
# and then use this to read
text_format.Parse(f.read(), address_book)
# and this to write
text_format.MessageToString(address_book)



Enjoy.</summary></entry><entry><title>Sphinx - python tips</title><link href="http://michal.sustr.sk/blog/python/sphinx-python-tips/" rel="alternate" type="text/html" title="Sphinx - python tips" /><published>2016-07-20T00:00:00+02:00</published><updated>2016-07-20T00:00:00+02:00</updated><id>http://michal.sustr.sk/blog/python/sphinx-python-tips</id><content type="html" xml:base="http://michal.sustr.sk/blog/python/sphinx-python-tips/">&lt;p&gt;I recently started to educate myself more in Python. I decided that from time to time I’m going to put here some tips and tricks, things that I’ve come across that I think are interesting. These are comments from a person who has quite a lot of experience with other languages, such as Java, PHP and Matlab. I’ll put up links to these articles on twitter under hashtag #pythontips, so if you like these you can just follow the hashtag.&lt;/p&gt;

&lt;h2 id=&quot;sphinx&quot;&gt;Sphinx&lt;/h2&gt;

&lt;p&gt;I started to work on a new project and since I have quite a free hand at what I’m doing, I decided to do things right. One of the most important things in any project is its documentation.&lt;/p&gt;

&lt;p&gt;I searched for what would be nice in Python, actually there’s a huge list of different &lt;a href=&quot;https://en.wikipedia.org/wiki/Comparison_of_documentation_generators&quot;&gt;documentation generator systems on wikipedia&lt;/a&gt; Searching on the web I found Sphinx, seemed quite nice so I gave it a try.&lt;/p&gt;

&lt;p&gt;In my conda installation all I had to do is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda install sphinx
# and then follow this set-up tutorial
$ sphinx-quickstart
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It is going to ask you bunch of questions - if you want to generate docs from code, don’t forget to allow &lt;strong&gt;autodoc&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Useful links:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Here is a link to tutorial: &lt;a href=&quot;https://codeandchaos.wordpress.com/2012/07/30/sphinx-autodoc-tutorial-for-dummies/&quot;&gt;https://codeandchaos.wordpress.com/2012/07/30/sphinx-autodoc-tutorial-for-dummies/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;How to write code for .rst files: &lt;a href=&quot;https://pythonhosted.org/an_example_pypi_project/sphinx.html&quot;&gt;https://pythonhosted.org/an_example_pypi_project/sphinx.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Use markdown instead of rst: &lt;a href=&quot;http://searchvoidstar.tumblr.com/post/125486358368/making-pdfs-from-markdown-on-readthedocsorg-using&quot;&gt;http://searchvoidstar.tumblr.com/post/125486358368/making-pdfs-from-markdown-on-readthedocsorg-using&lt;/a&gt;
(I didn’t get that to work properly yet, but others seem to have)&lt;/li&gt;
  &lt;li&gt;All kinds of options: &lt;a href=&quot;http://www.sphinx-doc.org/en/stable/domains.html&quot;&gt;http://www.sphinx-doc.org/en/stable/domains.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I set up my project with following structure&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;├── data
├── docs        # here is Sphinx documentation
│   ├── _build
│   ├── conf.py
│   ├── index.rst
│   ├── make.bat
│   ├── Makefile
│   ├── _static
│   └── _templates
├── Makefile
├── NOTES.md
├── README.md
├── requirements.txt
├── src              # my main code
├── temp
└── tests
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;conf.py&lt;/code&gt; has all of the Sphinx config, the one that you setup in the &lt;code class=&quot;highlighter-rouge&quot;&gt;spinhx-quickstart&lt;/code&gt;. Find &lt;code class=&quot;highlighter-rouge&quot;&gt;sys.path.insert&lt;/code&gt; and add &lt;code class=&quot;highlighter-rouge&quot;&gt;sys.path.insert(0, os.path.abspath('../src'))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;My main project Makefile has these scripts:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# I wanted to have it called docs, but that confuses make command
# since there is a directory there already called docs - these two have to be distinct.
doc: 
	# remove all old rst files except index.rst
	find ./docs -name '*.rst' ! -name 'index.rst' -type f -exec rm -f {} \;
	# generate new .rst files for docs
	sphinx-apidoc -l -F --module-first -o docs src
	# vrrrrruuummm!!! generate docs
	cd docs; make html;

showdoc:
	chromium-browser docs/_build/html/index.html
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;That’s it, I just type &lt;code class=&quot;highlighter-rouge&quot;&gt;make doc&lt;/code&gt; and then &lt;code class=&quot;highlighter-rouge&quot;&gt;make showdoc&lt;/code&gt; and the documentation is born. Voila!&lt;/p&gt;</content><author><name>Michal Šustr</name><uri>http://michal.sustr.sk</uri></author><summary>I recently started to educate myself more in Python. I decided that from time to time I’m going to put here some tips and tricks, things that I’ve come across that I think are interesting. These are comments from a person who has quite a lot of experience with other languages, such as Java, PHP and Matlab. I’ll put up links to these articles on twitter under hashtag #pythontips, so if you like these you can just follow the hashtag.

Sphinx

I started to work on a new project and since I have quite a free hand at what I’m doing, I decided to do things right. One of the most important things in any project is its documentation.

I searched for what would be nice in Python, actually there’s a huge list of different documentation generator systems on wikipedia Searching on the web I found Sphinx, seemed quite nice so I gave it a try.

In my conda installation all I had to do is:

$ conda install sphinx
# and then follow this set-up tutorial
$ sphinx-quickstart



It is going to ask you bunch of questions - if you want to generate docs from code, don’t forget to allow autodoc

Useful links:


  Here is a link to tutorial: https://codeandchaos.wordpress.com/2012/07/30/sphinx-autodoc-tutorial-for-dummies/
  How to write code for .rst files: https://pythonhosted.org/an_example_pypi_project/sphinx.html
  Use markdown instead of rst: http://searchvoidstar.tumblr.com/post/125486358368/making-pdfs-from-markdown-on-readthedocsorg-using
(I didn’t get that to work properly yet, but others seem to have)
  All kinds of options: http://www.sphinx-doc.org/en/stable/domains.html


I set up my project with following structure

├── data
├── docs        # here is Sphinx documentation
│   ├── _build
│   ├── conf.py
│   ├── index.rst
│   ├── make.bat
│   ├── Makefile
│   ├── _static
│   └── _templates
├── Makefile
├── NOTES.md
├── README.md
├── requirements.txt
├── src              # my main code
├── temp
└── tests



conf.py has all of the Sphinx config, the one that you setup in the spinhx-quickstart. Find sys.path.insert and add sys.path.insert(0, os.path.abspath('../src'))

My main project Makefile has these scripts:

# I wanted to have it called docs, but that confuses make command
# since there is a directory there already called docs - these two have to be distinct.
doc: 
	# remove all old rst files except index.rst
	find ./docs -name '*.rst' ! -name 'index.rst' -type f -exec rm -f {} \;
	# generate new .rst files for docs
	sphinx-apidoc -l -F --module-first -o docs src
	# vrrrrruuummm!!! generate docs
	cd docs; make html;

showdoc:
	chromium-browser docs/_build/html/index.html



That’s it, I just type make doc and then make showdoc and the documentation is born. Voila!</summary></entry><entry><title>Human learning is way more efficient in power consumption… for now.</title><link href="http://michal.sustr.sk/blog/neural%20networks/human-learning-is-waaay-more-efficient-in-power-consumption-for-now/" rel="alternate" type="text/html" title="Human learning is way more efficient in power consumption... for now." /><published>2016-06-20T00:00:00+02:00</published><updated>2016-06-20T00:00:00+02:00</updated><id>http://michal.sustr.sk/blog/neural%20networks/human-learning-is-waaay-more-efficient-in-power-consumption-for-now</id><content type="html" xml:base="http://michal.sustr.sk/blog/neural%20networks/human-learning-is-waaay-more-efficient-in-power-consumption-for-now/">&lt;p&gt;When I was eating dinner a simple thought occurred to me: is machine learning more power efficient than human learning… &lt;em&gt;from scratch&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;A neural network called &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;AlexNet&lt;/a&gt; was trained between five and six days on two GTX 580 3GB GPUs which have &lt;a href=&quot;http://www.bit-tech.net/hardware/graphics/2010/11/09/nvidia-geforce-gtx-580-review/8&quot;&gt;363W consumption&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So simply calculating,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W_{GPU} = (5+6)\times 24 \times 363 = 95832\text{Wh}.&lt;/script&gt;

&lt;p&gt;I found some site that describes &lt;a href=&quot;http://www.whfoods.com/genpage.php?tname=specialneed&amp;amp;dbid=7&quot;&gt;recommended energy intakes for newborn babies&lt;/a&gt;. After some funny discussion we came to conclusion with my mom that I might’ve been able to perform ImageNet when I would have been 3 years old, so if I take that data, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W_{human} = 550\times 6\times 30+700\times 6\times 30+1000\times 2\times 12 = 249000\text{cal} = 289.587 \text{Wh}.&lt;/script&gt;

&lt;p&gt;By comparing these I got to conclusions that humans are about &lt;strong&gt;300 times more power efficient&lt;/strong&gt; in learning from scratch. Of course, I took an old version of GPU training so that might differ nowadays (I didn’t find other GPU usages quickly, but I knew I would find them in Alex’s paper. Maybe someone can calculate this for other nets?).&lt;/p&gt;

&lt;p&gt;Let’s assume that energy efficiency has the same &lt;a href=&quot;https://www.technologyreview.com/s/425398/a-new-and-improved-moores-law/&quot;&gt;exponential as Moore’s law&lt;/a&gt; so that it halves every 18 months.
The &lt;a href=&quot;https://en.wikipedia.org/wiki/GeForce_500_series&quot;&gt;GTX 580 launched on 9 November 2010&lt;/a&gt; so it’s almost six years now, and the consumptions should be at the level of 363\times 2^{-4} = 22W. With this consumption it would take in total 6kW. By projecting these numbers human level power efficiency should be achieved by year 2022 - 6 years from now. And with better algorithms it’s probably going to be even sooner. Isn’t this exciting?&lt;/p&gt;</content><author><name>Michal Šustr</name><uri>http://michal.sustr.sk</uri></author><summary>When I was eating dinner a simple thought occurred to me: is machine learning more power efficient than human learning… from scratch?

A neural network called AlexNet was trained between five and six days on two GTX 580 3GB GPUs which have 363W consumption.

So simply calculating,



I found some site that describes recommended energy intakes for newborn babies. After some funny discussion we came to conclusion with my mom that I might’ve been able to perform ImageNet when I would have been 3 years old, so if I take that data, then



By comparing these I got to conclusions that humans are about 300 times more power efficient in learning from scratch. Of course, I took an old version of GPU training so that might differ nowadays (I didn’t find other GPU usages quickly, but I knew I would find them in Alex’s paper. Maybe someone can calculate this for other nets?).

Let’s assume that energy efficiency has the same exponential as Moore’s law so that it halves every 18 months.
The GTX 580 launched on 9 November 2010 so it’s almost six years now, and the consumptions should be at the level of 363\times 2^{-4} = 22W. With this consumption it would take in total 6kW. By projecting these numbers human level power efficiency should be achieved by year 2022 - 6 years from now. And with better algorithms it’s probably going to be even sooner. Isn’t this exciting?</summary></entry><entry><title>Predpovede na rok 2016</title><link href="http://michal.sustr.sk/blog/predpovede-na-rok-2016/" rel="alternate" type="text/html" title="Predpovede na rok 2016" /><published>2015-12-23T00:00:00+01:00</published><updated>2015-12-23T00:00:00+01:00</updated><id>http://michal.sustr.sk/blog/predpovede-na-rok-2016</id><content type="html" xml:base="http://michal.sustr.sk/blog/predpovede-na-rok-2016/">&lt;p&gt;Chcem sa zahrať na futurológa :-) tak skúsim predpovedať čo sa stane v roku 2016
(hlavne v technológiach):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;pomocou deep learningu sa dostanú do áut prvé zariadenia ktoré budú robustne vykonávať jednoduché riadenie, napr. na ďiaľniciach. Pomôcť tomu môžu projekty ako &lt;a href=&quot;http://www.technologyreview.com/view/544926/ai-machine-learns-to-drive-using-crowdteaching/&quot;&gt;http://www.technologyreview.com/view/544926/ai-machine-learns-to-drive-using-crowdteaching/&lt;/a&gt; alebo &lt;a href=&quot;https://www.youtube.com/watch?v=KTrgRYa2wbI&quot;&gt;https://www.youtube.com/watch?v=KTrgRYa2wbI&lt;/a&gt; hoci Tesla si myslí že je taký systém príliš jednoduchý &lt;a href=&quot;http://www.techinsider.io/elon-musk-responds-to-george-hotz-bloomberg-story-2015-12&quot;&gt;http://www.techinsider.io/elon-musk-responds-to-george-hotz-bloomberg-story-2015-12&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;čoraz väčšie množstvo firiem začne používať AI, zdroje ktoré uvoľňuje FB a Google sú úplne neskutočné, napr. nedávno ich &lt;a href=&quot;https://www.google.sk/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=6&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwj8puG_tfLJAhULiywKHei0BOcQFgg0MAU&amp;amp;url=http%3A%2F%2Fwww.wired.com%2F2015%2F11%2Fgoogle-open-sources-its-artificial-intelligence-engine%2F&amp;amp;usg=AFQjCNE8kMa9RVbyZw1ApxW9z9Yy7-LceA&amp;amp;bvm=bv.110151844,d.bGg&quot;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;budú sa inkrementálne zlepšovať modely sietí, zlepšovať hardvér, zväčšovať hĺbka, ale nejaký breakthrough v podobe zaujímavého iného prístupu (ako napr. spôsobil AlexNet) ešte nenastane, nakoľko sa možnosti toho čo sa dá s týmito modelmi ešte všetko robiť nevyčerpalo.&lt;/li&gt;
  &lt;li&gt;bude zdvojnásobné množstvo qubitov na kvantových počítačoch&lt;/li&gt;
  &lt;li&gt;vláda U.S. a U.K. sa bude ďalej snažiť obmedzovať šifrovanie na internete pomocou vpašovávania rôznych debilných zákonov&lt;/li&gt;
  &lt;li&gt;prvýkrát vznikajú systémy ktoré sú schopné robustného mass-surveillance, čo je hrozivé, zvlášť pri rôznych iniciatívach typu &lt;a href=&quot;https://www.youtube.com/watch?v=lHcTKWiZ8sI&quot;&gt;https://www.youtube.com/watch?v=lHcTKWiZ8sI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;začnú vznikať rôzne startupy okolo virtuálnej reality, využije toho spočiatku porno priemysel, hry, a časom prídu aj inžinierske nástroje (stavbárina a pod.)&lt;/li&gt;
  &lt;li&gt;po prezidentských voľbách 2016 v USA praskne startupová bublina, podobne ako dot-com bublina padla v čase po prezidentských voľbách v roku 2000&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Michal Šustr</name><uri>http://michal.sustr.sk</uri></author><summary>Chcem sa zahrať na futurológa :-) tak skúsim predpovedať čo sa stane v roku 2016
(hlavne v technológiach):


  pomocou deep learningu sa dostanú do áut prvé zariadenia ktoré budú robustne vykonávať jednoduché riadenie, napr. na ďiaľniciach. Pomôcť tomu môžu projekty ako http://www.technologyreview.com/view/544926/ai-machine-learns-to-drive-using-crowdteaching/ alebo https://www.youtube.com/watch?v=KTrgRYa2wbI hoci Tesla si myslí že je taký systém príliš jednoduchý http://www.techinsider.io/elon-musk-responds-to-george-hotz-bloomberg-story-2015-12
  čoraz väčšie množstvo firiem začne používať AI, zdroje ktoré uvoľňuje FB a Google sú úplne neskutočné, napr. nedávno ich TensorFlow
  budú sa inkrementálne zlepšovať modely sietí, zlepšovať hardvér, zväčšovať hĺbka, ale nejaký breakthrough v podobe zaujímavého iného prístupu (ako napr. spôsobil AlexNet) ešte nenastane, nakoľko sa možnosti toho čo sa dá s týmito modelmi ešte všetko robiť nevyčerpalo.
  bude zdvojnásobné množstvo qubitov na kvantových počítačoch
  vláda U.S. a U.K. sa bude ďalej snažiť obmedzovať šifrovanie na internete pomocou vpašovávania rôznych debilných zákonov
  prvýkrát vznikajú systémy ktoré sú schopné robustného mass-surveillance, čo je hrozivé, zvlášť pri rôznych iniciatívach typu https://www.youtube.com/watch?v=lHcTKWiZ8sI
  začnú vznikať rôzne startupy okolo virtuálnej reality, využije toho spočiatku porno priemysel, hry, a časom prídu aj inžinierske nástroje (stavbárina a pod.)
  po prezidentských voľbách 2016 v USA praskne startupová bublina, podobne ako dot-com bublina padla v čase po prezidentských voľbách v roku 2000</summary></entry></feed>
